diff --git a/.gitignore b/.gitignore
index e14e07f..45bcfe2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -163,7 +163,7 @@ cython_debug/
 
 # Data folders
 data/
-experts/
+models/
 checkpoints/
 
 # Configuration file
diff --git a/src/bmetrics/__main__.py b/src/bmetrics/__main__.py
index 9586112..464b87f 100644
--- a/src/bmetrics/__main__.py
+++ b/src/bmetrics/__main__.py
@@ -1,5 +1,4 @@
 import toml
-from itertools import product
 import torch
 from fairchem.core.datasets import LmdbDataset
 from torch.utils.data import Subset
@@ -7,9 +6,7 @@ from torch.utils.data import Subset
 import wandb
 from bmetrics.config import Config
 from bmetrics.dataset import split_train_val_test
-from bmetrics.pretrained_models import load_experts
 from bmetrics.train import make_trainer
-from bmetrics.models import make_moe
 
 
 def main():
@@ -21,44 +18,10 @@ def main():
     if not config.subset_size == 0:
         dataset = Subset(dataset, indices=list(range(config.subset_size)))
     dataloaders = split_train_val_test(dataset, config)
-    experiments = product(config.experiments.ensemble, config.experiments.fine_tune)
-    experts = load_experts(
-        config.experiments.expert_names, config.paths.experts, config.device
-    )
-    for ensemble, fine_tune, names in experiments:
-        model = make_model(ensemble, fine_tune, names)
-        experts = load_experts(
-            names=names, experts_path=config.paths.experts, device=config.device
-        )
-        if ensemble == "single":
-            for model in experts:
-                if fine_tune:
-                    trainer = make_trainer(
-                        config=config,
-                        model=model,
-                        ensemble=ensemble,
-                        dataloaders=dataloaders,
-                    )
-                    trainer.train()
-                test_loss = trainer.test()
-                print(f"Test Loss: {test_loss:.4f}")
-        elif ensemble == "average":
-            print("Pass")
-            pass
-        elif ensemble == "moe":
-            model = make_moe(config=config, experts=experts)
-            if fine_tune:
-                trainer = make_trainer(
-                    config=config,
-                    model=model,
-                    ensemble=ensemble,
-                    dataloaders=dataloaders,
-                )
-                trainer.train()
-            test_loss = trainer.test()
-            print(f"Test Loss: {test_loss:.4f}")
-        else:
-            raise ValueError(f"Model {model} not recognized")
+    trainer = make_trainer(config=config, dataloaders=dataloaders)
+    trainer.train()
+    test_loss = trainer.test()
+    print(f"Test Loss: {test_loss:.4f}")
     wandb.finish()
 
 
diff --git a/src/bmetrics/config.py b/src/bmetrics/config.py
index 15fa729..1c1e74a 100644
--- a/src/bmetrics/config.py
+++ b/src/bmetrics/config.py
@@ -3,15 +3,9 @@ from pathlib import Path
 from pydantic import BaseModel
 
 
-class Experiments(BaseModel):
-    expert_names: list[str]
-    ensemble: list[str]
-    fine_tune: list[bool]
-
-
 class Paths(BaseModel):
     data: Path
-    experts: Path
+    models: Path
     checkpoints: Path
 
 
@@ -23,6 +17,7 @@ class DataloaderConfig(BaseModel):
 
 
 class ModelConfig(BaseModel):
+    names: list[str]
     hidden_dim: int
     input_dim: int
     num_layers: int
@@ -47,7 +42,6 @@ class Config(BaseModel):
     random_seed: int
     subset_size: int  # 0 means no subset
     device: str
-    experiments: Experiments
     dataloader: DataloaderConfig
     model: ModelConfig
     optimizer: OptimizerConfig
diff --git a/src/bmetrics/models.py b/src/bmetrics/models.py
index 173a3bd..4f1b948 100644
--- a/src/bmetrics/models.py
+++ b/src/bmetrics/models.py
@@ -4,7 +4,7 @@ import torch.nn.functional as F
 from torch_geometric.nn import GCNConv, global_mean_pool
 
 from bmetrics.config import Config
-from bmetrics.pretrained_models import get_expert_output
+from bmetrics.pretrained_models import get_expert_output, load_experts
 
 
 class GatingGCN(torch.nn.Module):
@@ -53,7 +53,12 @@ class MixtureOfExperts(nn.Module):
         return prediction
 
 
-def make_moe(config: Config, experts: list[nn.Module]) -> MixtureOfExperts:
+def make_moe(config: Config):
+    experts = load_experts(
+        model_names=config.model.names,
+        models_path=config.paths.models,
+        device=config.device,
+    )
     gating_network = GatingGCN(
         **config.model.model_dump(exclude={"names"}), experts=experts
     )
diff --git a/src/bmetrics/pretrained_models.py b/src/bmetrics/pretrained_models.py
index 4fa83af..93372e9 100644
--- a/src/bmetrics/pretrained_models.py
+++ b/src/bmetrics/pretrained_models.py
@@ -4,38 +4,15 @@ import toml
 import torch
 from fairchem.core.models.dimenet_plus_plus import DimeNetPlusPlus, DimeNetPlusPlusWrap
 import torch.nn as nn
-from fairchem.core.models.painn.painn import PaiNN
+from fairchem.core.models.equiformer_v2.equiformer_v2 import (
+    EquiformerV2Backbone,
+    EquiformerV2EnergyHead,
+)
+from fairchem.core.models.painn import PaiNN
 from fairchem.core.models.schnet import SchNet, SchNetWrap
 
 
-class BMetricsDimeNetPlusPlus(nn.Module):
-    def __init__(self, model):
-        super().__init__()
-        self.model = model
-
-    def forward(self, data):
-        return self.model(data)["energy"]
-
-
-class BMetricsSchNet(nn.Module):
-    def __init__(self, model):
-        super().__init__()
-        self.model = model
-
-    def forward(self, data):
-        return self.model(data)["energy"]
-
-
-class BMetricPaiNN(PaiNN):  # type: ignore
-    def __init__(self, model):
-        super().__init__()
-        self.model = model
-
-    def forward(self, data):
-        return self.model(data)["energy"].unsqueeze(1)
-
-
-def instantiate_model(
+def set_up_model(
     model_class,
     model_arguments: dict,
     weights_path: str,
@@ -49,31 +26,46 @@ def instantiate_model(
     return model
 
 
-MODEL_CLASSES = {
-    "dimenetpp": (DimeNetPlusPlusWrap, , "dimenetpp_all.pt"),
-    "schnet": (SchNetWrap, "schnet_all_large.pt"),
-    "painn": (PaiNN, "painn_all.pt"),
-}
-
-
-def load_expert(name: str, config: dict, experts_path: Path, device: str) -> nn.Module:
-    model_class, weights_filename = MODEL_CLASSES[name]
-    weights_path = experts_path / weights_filename
-    model = instantiate_model(
-        model_class=model_class,
-        model_arguments=config[name],
-        weights_path=weights_path,
-        device=device,
-    )
-    return model
-
-
-def load_experts(names: list, experts_path: Path, device: str) -> list[nn.Module]:
+def load_experts(model_names: list, models_path: Path, device: str) -> list[nn.Module]:
     config = toml.load("pretrained.toml")
-    experts = [
-        load_expert(name=name, config=config, experts_path=experts_path, device=device)
-        for name in names
-    ]
+    experts = []
+    if "dimenetpp" in model_names:
+        weights_path = f"{models_path}/dimenetpp_all.pt"
+        model = set_up_model(
+            model_class=DimeNetPlusPlusWrap,
+            model_arguments=config["dimenetpp"],
+            weights_path=weights_path,
+            device=device,
+        )
+        experts.append(model)
+
+    if "schnet" in model_names:
+        weights_path = f"{models_path}/schnet_all_large.pt"
+        model = set_up_model(
+            model_class=SchNetWrap,
+            model_arguments=config["schnet"],
+            weights_path=weights_path,
+            device=device,
+        )
+        experts.append(model)
+    if "painn" in model_names:
+        weights_path = f"{models_path}/painn/painn_all.pt"
+        model = set_up_model(
+            model_class=PaiNN,
+            model_arguments=config["painn"],
+            weights_path=weights_path,
+            device=device,
+        )
+        experts.append(model)
+    if "equiformerv2" in model_names:
+        weights_path = f"{models_path}/eq2_153M_ec4_allmd.pt"
+        model = set_up_model(
+            model_class=EquiformerV2Backbone,
+            model_arguments=config["equiformerv2"],
+            weights_path=weights_path,
+            device=device,
+        )
+        experts.append(model)
     return experts
 
 
@@ -93,5 +85,10 @@ def get_expert_output(data, model):
         case PaiNN():  # type: ignore
             prediction = model(data)["energy"]
             return prediction.unsqueeze(1)
+        case EquiformerV2Backbone():  # type: ignore
+            energy_head = EquiformerV2EnergyHead(model)
+            emb = model(data)
+            prediction = energy_head(data=data, emb=emb)["energy"].unsqueeze(1)
+            return prediction
         case _:
             raise ValueError(f"Model '{model}' not recognized")
diff --git a/src/bmetrics/train.py b/src/bmetrics/train.py
index 9b38917..6862f52 100644
--- a/src/bmetrics/train.py
+++ b/src/bmetrics/train.py
@@ -4,14 +4,13 @@ from torch_geometric.loader import DataLoader
 
 import wandb
 from bmetrics.config import Config
-from bmetrics.models import get_expert_output
+from bmetrics.models import make_moe
 from bmetrics.dataset import DataloaderSplits
 
 
 class Trainer:
     def __init__(
         self,
-        ensemble: str,
         model: torch.nn.Module,
         criterion: nn.MSELoss,
         optimizer: optim.Optimizer,
@@ -22,7 +21,6 @@ class Trainer:
         config: Config,
     ):
         self.model = model
-        self.ensemble = ensemble
         self.criterion = criterion
         self.optimizer = optimizer
         self.scheduler = scheduler
@@ -32,20 +30,10 @@ class Trainer:
         self.best_val_loss = float("inf")
         self.config = config
 
-    def route_ensemble_output(self, data):
-        if self.ensemble == "single":
-            return get_expert_output(data, self.model)
-        elif self.ensemble == "average":
-            pass
-        elif self.ensemble == "moe":
-            return self.model(data)
-        else:
-            raise ValueError(f"Ensemble {self.ensemble}, not recognized")
-
     def train_step(self, data):
         data = data.to(self.config.device)
         self.optimizer.zero_grad()
-        pred = self.route_ensemble_output(data)
+        pred = self.model(data)
         loss = self.criterion(pred, data.energy)
         loss.backward()
         torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
@@ -89,7 +77,7 @@ class Trainer:
         loss = 0.0
         for data in dataloader:
             data = data.to(self.config.device)
-            pred = self.route_ensemble_output(data)
+            pred = self.model(data)
             loss = self.criterion(pred, data.energy)
             loss += loss.item()
         loss /= len(dataloader)
@@ -102,9 +90,8 @@ class Trainer:
         return self.evaluate(self.test_loader)
 
 
-def make_trainer(
-    config: Config, model: nn.Module, ensemble: str, dataloaders: DataloaderSplits
-) -> Trainer:
+def make_trainer(config: Config, dataloaders: DataloaderSplits) -> Trainer:
+    model = make_moe(config)
     criterion = nn.MSELoss()
     optimizer = optim.SGD(model.parameters(), **config.optimizer.model_dump())
     scheduler = optim.lr_scheduler.CosineAnnealingLR(
@@ -112,7 +99,6 @@ def make_trainer(
     )
     trainer = Trainer(
         model=model,
-        ensemble=ensemble,
         criterion=criterion,
         optimizer=optimizer,
         scheduler=scheduler,
